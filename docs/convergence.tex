\documentclass[11pt]{article}

\usepackage{amsmath, amsfonts, amsthm, amssymb, mathtools}
\usepackage{authblk, color, bm, graphicx, epstopdf, url}
\usepackage[font = small, labelfont = bf, labelsep = period]{caption}
\usepackage{subcaption}
\usepackage{xspace}
\usepackage{enumitem,hyperref}
\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}

\usepackage{booktabs, tabularx, multirow}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\newcolumntype{R}{>{\raggedleft\arraybackslash}X}
\newcolumntype{L}{>{\raggedright\arraybackslash}X}

\newtheorem{prop}{Proposition}
\newtheorem{obs}{Observation}
\newtheorem{rem}{Remark}
\newtheorem{ques}{Question}

\newcommand{\comment}[1]{{\color{red}#1}}
\newcommand{\Image}[1]{\mathop{\text{Im}}\left(#1\right)}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

\usepackage{fullpage}[2cm]
\linespread{1.5}

\allowdisplaybreaks

\title{Jacobi decomposition of the proximal augmented lagrangian}
\author{Anirudh Subramanyam, Youngdae Kim, Mihai Anitescu}
\date{\today}

\begin{document}
\maketitle

\section{Formulation}
We are interested in solving the following problem.
\begin{equation}\label{eq:nlp}
\begin{array}{r@{\;\;}l}
\displaystyle\mathop{\text{minimize}}_{x_1, \ldots, x_N} & \displaystyle \sum_{i = 1}^N f_i(x_i) \\
\text{subject to} & \displaystyle x_i \in \mathbb{R}^{n_i}, \;\; i \in [N] \coloneqq \{1, \ldots, N\}, \\
& \displaystyle h_i(x_i) = 0, \;\; i \in [N] \\
& \displaystyle \sum_{i = 1}^N A_i x_i = b
\end{array}
\end{equation}
where $A_i \in \mathbb{R}^{m \times n_i}$, $f_i: \mathbb{R}^{n_i} \mapsto \mathbb{R}$ and $h_i :\mathbb{R}^{n_i} \mapsto \mathbb{R}^{c_i}$. %For convenience, we denote by $x$ to mean all the decision variables $(x_1, \ldots, x_N)$.

\subsection{Assumptions}
\begin{enumerate}
    \item\label{assume:feasible}
    Problem~\eqref{eq:nlp} is feasible and bounded.
    \item\label{assume:differentiable}
    All functions $f_i$ and $h_i$ are continuously differentiable. 
    \item\label{assume:lipschitz}
    $\nabla f_i$ and $\nabla h_i$ are Lipschitz continuous with constant $L$.
    \item\label{assume:bounded}
    $\nabla f_i$ and $\nabla h_i$ are uniformly bounded with bound $B$.
    \item\label{assume:licq}
    Let $C_i = (\nabla h_i)^\top : \mathbb{R}^{n_i} \mapsto \mathbb{R}^{n_i \times c_i}$ denote the constraint Jacobian. We assume $C_i$ has full rank for all $x_i \in \mathbb{R}^{n_i}$. \comment{Can we relax this?}
    \item\label{assume:coupling}
    There exists an index $\in [N]$ (w.l.o.g, let this be the last index $N$) such that the linear system (in variables $v$), $A_N \cdot v = b - \sum_{i = 1}^{N-1} A_i y_i$ is always solvable for any $y_i$. In other words, $b - \sum_{i = 1}^{N-1} A_i y_i \in \Image{A_N}$ for all $y_i \in \mathbb{R}^{n_i}$,  $i = 1, \ldots, N-1$. \comment{Can we relax this?}
\end{enumerate}

\section{Algorithm}
Choose values for the following parameters.
\begin{itemize}
    \item $\rho > 0$: the augmented Lagrangian coefficient
    \item $w_1, \ldots, w_N > 0$: the proximal coefficients
    \item $x^0$: initial guess (must be feasible. \comment{Can we relax this?})
    \item $\lambda^0 \in \mathbb{R}^{m}$: initial guess for the dual variable corresponding to $\sum_{i = 1}^N A_i x_i = b$.
\end{itemize}

The algorithm works as follows. Set $k = 0$.
\begin{enumerate}
    \item For $i = 1, \ldots, N$: compute a local solution $x_i^k$ of the following problem.
    \begin{equation}\label{eq:primalstep}
    \begin{array}{r@{\;\;}l}
    \displaystyle\mathop{\text{minimize}}_{x_i} & \displaystyle f_i(x_i) + (\lambda^{k-1})^\top \left[ A_ix_i + \sum_{j \neq i}^N A_j x_j^{k-1} - b\right] + \frac{\rho}{2} \norm{A_ix_i + \sum_{j \neq i}^N A_j x_j^{k-1} - b}^2  \\
    &\displaystyle + \frac{w_i}{2} \norm{x_i - x_i^{k-1}}^2\\
    \text{subject to} & \displaystyle  h_i(x_i) = 0.
    \end{array}
    \end{equation}
    
    \item Set 
    \begin{equation}\label{eq:dualstep}
    \lambda^k = \lambda^{k - 1} + \rho \left[\sum_{i = 1}^N A_i x_i^k - b\right]
    \end{equation}
    Set $k = k + 1$ and to step~1.
\end{enumerate}

\section{Convergence analysis}
Before we make our claim, observe that under Assumption~\ref{assume:licq}, any local solution $x^*$ of~\eqref{eq:nlp} satisfies the KKT conditions: there exist $\mu_i^* \in \mathbb{R}^{c_i}$ and $\lambda^* \in \mathbb{R}^m$ such that the following holds.
\begin{subequations}\label{eq:kkt_nlp}
\begin{align}
& h_i(x_i^*) = 0, \;\; i \in [N] \label{eq:kkt_nlp:primalfeas}\\
& \sum_{i = 1}^N A_i x_i^* = b \label{eq:kkt_nlp:coupling}\\
& \nabla f_i(x_i^*) + C_i(x_i^*)^\top \mu_i^* + A_i^\top \lambda^* = 0, \;\; i \in [N]\label{eq:kkt_nlp:dualfeas}
\end{align}
\end{subequations}
Equation~\eqref{eq:kkt_nlp:primalfeas} is always satisfied by the solutions $x^k$ generated by the Algorithm. The following proposition claims that the other two equations are also satisfied, in the limit $k \to \infty$.

\begin{prop}
    Let $(x^k, \lambda^k)$ be the sequence of solutions generated by the Algorithm. Let $\mu^k$ be the Lagrange multipliers corresponding to $h_i(x_i) = 0$ in Step~1 of the Algorithm. Then, we have
    \begin{subequations}\label{eq:kkt_lim}
        \begin{align}
        & \lim_{k \to \infty} \norm{\sum_{i = 1}^N A_i x_i^k - b} = 0 \label{eq:kkt_lim:coupling}\\
        & \lim_{k \to \infty} \norm{\nabla f_i(x_i^k) + C_i(x_i^k)^\top \mu_i^k + A_i^\top \lambda^k} = 0\label{eq:kkt_lim:dualfeas}
        \end{align}
    \end{subequations}
\end{prop}


\section{Proof sketch}
By Assumption~\eqref{assume:licq}, $x_i^k$ generated in Step~1 must satisfy the KKT conditions:
\begin{equation*}
\nabla f_i(x_i^k) + C_i(x_i^k)^\top \mu_i^k + A_i^\top \lambda^{k - 1} + \rho A_i^\top \left[ A_ix_i^k + \sum_{j \neq i}^N A_j x_j^{k-1} - b\right] + w_i [x_i^k - x_i^{k-1}] = 0.
\end{equation*}
Substitute for $\lambda^{k-1}$ in the above equation, using~\eqref{eq:dualstep}. We get:
\begin{equation}\label{eq:kkt_primalstep}
\nabla f_i(x_i^k) + C_i(x_i^k)^\top \mu_i^k + A_i^\top \lambda^k \underset{\text{\normalsize $\coloneqq R_i^k$}}{\underbrace{- \rho A_i^\top \sum_{j \neq i}^N A_j (x_j^k - x_j^{k - 1}) + w_i(x_i^k - x_i^{k-1})}} = 0.
\end{equation}

Observe that, if we can ensure $\norm{R_i^k} \to 0$, then we will satisfy condition~\eqref{eq:kkt_lim:dualfeas}.
Denote $\Delta x_i^k \coloneqq x_i^k - x_i^{k - 1}$ for all $i$ and $k$. Then, observe that the triangle and Cauchy-Schwarz inequalities imply 
\begin{equation*}
\norm{R_i^k} \leq  \sum_{j \neq i}^N \rho \norm{A_i^\top A_j} \norm{\Delta x_j^k} + w_i\norm{\Delta x_i^k}
\end{equation*}
Therefore, our goal will be to try and show that $\Delta x_i^k \to 0$ for all $i \in [N]$.

Similarly, if we can ensure $\norm{\Delta \lambda^k} \to 0$, where $\Delta \lambda^k \coloneqq \lambda^k - \lambda^{k-1}$ (this quantity is equal to $\rho \big[\sum_{i = 1}^N A_i x_i^k - b\big]$ because of Step~2 of the algorithm), then we will satisfy condition~\eqref{eq:kkt_lim:coupling}.
In Section~\ref{sec:proof:lambda_to_x}, we reduce this condition to be equivalent to $\Delta x^k \to 0$.
In Section~\ref{sec:proof:lyapunov}, we exhibit a bounded Lyapunov sequence that decreases by a fixed positive multiple of $\norm{\Delta x^k}$ in each step.
As the sequence is bounded from below, we must have $\norm{\Delta x^k} \to 0$ as $k \to \infty$, thus proving Proposition~1.

\subsection{Showing $\Delta \lambda^k \to 0$ is equivalent to showing $\Delta x^k \to 0$}\label{sec:proof:lambda_to_x}
From~\eqref{eq:dualstep} and using Assumption~\ref{assume:coupling}, we have:
\begin{align}
\Delta \lambda^k = \rho\left[\underset{\in \Image{A_N}}{\underbrace{\sum_{i = 1}^{N-1} A_ix_i^k - b}} + \underset{\in \Image{A_N}}{\underbrace{A_N x_N^k}}\right] \in \Image{A_N}
&\implies \text{Proj}_{\text{Ker}(A_N^\top)} \Delta \lambda^k = 0 \notag \\
&\implies \norm{A_N^\top \Delta \lambda^k} \geq \sigma^{+}_{\text{min}}(A_N^\top) \norm{\Delta \lambda^k} \coloneqq S^{-1} \norm{\Delta \lambda^k}. \tag{*} \label{eq:temp1}
\end{align}
where $S \coloneqq \sigma^{+}_{\text{min}}(A_N^\top) > 0$ denotes the smallest positive singular value of $A_N^\top$.
Next, from~\eqref{eq:kkt_primalstep} for $i = N$, and by definition of $\Delta \lambda^k \coloneqq \lambda^k - \lambda^{k-1}$, we have:
\begin{equation*}
-A_N^\top \Delta \lambda^k =
\underset{1^\text{st} \text{ term}}{\underbrace{\big[\nabla f_N(x_N^k) - \nabla f_N(x_N^{k-1})\big]}}
+ \underset{2^\text{nd} \text{ term}}{\underbrace{\big[C_N(x_N^k)^\top \mu_N^k - C_N(x_N^{k-1})^\top \mu_N^{k-1}\big]}}
+ \underset{3^\text{rd} \text{ term}}{\underbrace{\big[R_N^k - R_N^{k-1}\big]}}
\end{equation*}
Using the triangle inequality and combining with~\eqref{eq:temp1}, we have:
\begin{equation*}
\norm{\Delta^k} \leq S\norm{A_N^\top \Delta \lambda^k} \leq S\cdot\left(\norm{1^\text{st} \text{ term}} + \norm{2^\text{nd} \text{ term}} + \norm{3^\text{rd} \text{ term}}\right)
\end{equation*}

Now, we analyze each of the three terms using Assumptions~\eqref{assume:lipschitz} and~\eqref{assume:bounded}.
\begin{equation*}
\norm{1^\text{st} \text{ term}} \leq L \norm{\Delta x_N^k}
\end{equation*}
\begin{align*}
2^\text{nd} \text{ term} &= \sum_{l = 1}^{c_N} \big[\mu_{Nl}^k\nabla h_{Nl}(x_N^k) - \mu_{Nl}^{k-1}\nabla h_{Nl}(x_N^{k-1}) \big] \\
\implies  \norm{2^\text{nd} \text{ term}} &\leq \sum_{l = 1}^{c_N} \norm{\mu_{Nl}^k\nabla h_{Nl}(x_N^k) - \mu_{Nl}^{k-1}\nabla h_{Nl}(x_N^{k-1})} \\
&= \sum_{l = 1}^{c_N} \norm{\mu_{Nl}^k\nabla h_{Nl}(x_N^k) - \mu_{Nl}^k\nabla h_{Nl}(x_N^{k-1}) - \mu_{Nl}^{k-1}\nabla h_{Nl}(x_N^{k-1}) + \mu_{Nl}^{k}\nabla h_{Nl}(x_N^{k-1})} \\
%&\leq \sum_{l = 1}^{c_N} \left[\norm{\mu_{Nl}^k\nabla h_{Nl}(x_N^k) - \mu_{Nl}^k\nabla h_{Nl}(x_N^{k-1})} + \norm{\mu_{Nl}^{k-1}\nabla h_{Nl}(x_N^{k-1}) + \mu_{Nl}^{k}\nabla h_{Nl}(x_N^{k-1})}\right] \\
&\leq \sum_{l = 1}^{c_N} \left[\norm{\mu_{Nl}^k} \norm{\nabla h_{Nl}(x_N^k) - \nabla h_{Nl}(x_N^{k-1})} + \norm{\nabla h_{Nl}(x_N^{k-1})} \underset{\coloneqq\Delta \mu_{Nl}^k}{\underbrace{\norm{\mu_{Nl}^{k} - \mu_{Nl}^{k-1}}}}\right] \\
%&\leq \sum_{l = 1}^{c_N} \left[L \norm{\mu_{Nl}^k} \norm{\Delta x_N^k} + B\norm{\Delta \mu_{Nl}^{k}}\right] \\
&\leq L \norm{\mu_{N}^k}_1 \norm{\Delta x_N^k} + B\norm{\Delta \mu_{N}^{k}}_1
\end{align*}
\begin{equation*}
\norm{3^\text{rd} \text{ term}} \leq \rho \sum_{j = 1}^{N-1} \norm{A_N^\top A_j} \left(\norm{\Delta x_j^k} + \norm{\Delta x_j^{k - 1}} \right) + w_N\left(\norm{\Delta x_N^k} + \norm{\Delta x_N^{k-1}}\right)
\end{equation*}

{\color{magenta}At this point, I realized that I made a mistake in my derivation. In short, I am not able to bound the norm of the Lagrange multipliers $\norm{\mu_{N}^k}_1$ which is needed later on in the proof. The only way out is to make another assumption, which makes the $2^\text{nd}$ term disappear.
\begin{enumerate}\setcounter{enumi}{6}
    \item\label{assume:unconstrained} Assume that $h_N \equiv 0$.
\end{enumerate}
In other words, this assumption says that there exists a block of variables, $x_N$ in this case, which are essentially unconstrained except for the coupling constraints.
This significantly weakens the proof, and makes it quite similar to existing analyses of ADMM for non-convex problems.

\begin{itemize}
    \item Note that if the assumption is not already satisfied, then it can be ensured by introducing additional slack variables $z$ and modifying the constraints $\sum_{i = 1}^N A_i x_i = b - z$ and the objective $\sum_{i = 1}^N f_i(x_i) + M \norm{z}^2$.
    These modifications will ensure that Assumption~\eqref{assume:coupling} continues to be satisfied.
    However, I don't know how to calculate the penalty parameter $M$ that ensures equivalence with the original problem.
    
    \item Alternatively, it would be nice to extend this at least for the case where there are bound constraints on $x_N$. If we can do this, then we can extend the theory to \emph{coupled inequality constraints}: $\sum_{i = 1}^N A_i x_i \leq b$, where the $m \times (n_1 + \ldots + n_N)$ matrix $\left[A_1 \; \ldots \; A_N\right]$ is full rank. In this case, we can introduce a new block of slack variables $z \geq 0$ and make the inequalities to be equalities: $\sum_{i = 1}^N A_i x_i + z = b$. Now, note that the variable block $z$ satisfies both Assumption~\ref{assume:coupling} and Assumption~\ref{assume:unconstrained} and the theory should (hopefully) go through.
    
    Note that this latter case is exactly what happens for both the multi-period and contingency-constrained OPF.
\end{itemize}
}

Putting it all together, we have the following (note that we ignored the $2^\text{nd}$ term):
\begin{equation}\label{eq:delta_lambda}
\norm{\Delta \lambda^k} \leq SL \norm{\Delta x_N^k} + S\rho \sum_{j = 1}^{N-1} \norm{A_N^\top A_j} \left(\norm{\Delta x_j^k} + \norm{\Delta x_j^{k - 1}} \right) + Sw_N\left(\norm{\Delta x_N^k} + \norm{\Delta x_N^{k-1}}\right)
\end{equation}




\subsection{Bounded Lyapunov sequence that decreases by $\norm{\Delta x^k}$ in each step}\label{sec:proof:lyapunov}
Define $L(x, \lambda) : \mathbb{R}^{n_1 + \ldots + n_N} \times \mathbb{R}^m \mapsto \mathbb{R}$ to be the following function (it is the Augmented Lagrangian of problem~\eqref{eq:nlp} with respect to the coupling constraints):
\begin{equation}\label{eq:L_def}
L(x, \lambda)
\; = \;
\sum_{i = 1}^N f_i(x_i) + \lambda^\top \left[ \sum_{i = 1}^N A_ix_i - b\right] + \frac{\rho}{2} \norm{\sum_{i=1}^N A_ix_i - b}^2
\end{equation}

Similar to existing ADMM analyses, we now define the following Lyapunov sequence:
\begin{equation}\label{eq:lyapunov_sequence_definition}
\mathcal{L}^k \coloneqq L(x^k, \lambda^k) + \frac{w_N}{4} \norm{\Delta x_N^k}^2 
\end{equation}
In subsection~\ref{sec:lyapunov_monotone}, we shall prove that this sequence decreases monotonically at least by some positive factor proportional to $\norm{\Delta x^k}$.
In subsection~\ref{sec:lyapunov_bounded}, we shall prove that the sequence is bounded from below.

\subsubsection{Monotonicity}\label{sec:lyapunov_monotone}
We study $\mathcal{L}^k - \mathcal{L}^{k-1}$. For that, first consider
\begin{align*}
L(x^k, \lambda^k) - L(x^{k-1}, \lambda^{k-1})
=
\underset{1^\text{st} \text{ term}}{\underbrace{\big[L(x^k, \lambda^k) - L(x^k, \lambda^{k-1})\big]}}
+
\underset{2^\text{nd} \text{ term}}{\underbrace{\big[L(x^k, \lambda^{k-1}) - L(x^{k-1}, \lambda^{k-1})\big]}}
\end{align*}

The dual update step~\eqref{eq:dualstep} and inequality~\eqref{eq:delta_lambda} implies that:
\begin{align*}
\norm{1^\text{st} \text{ term}} &= \frac{1}{\rho}\norm{\Delta \lambda^k}^2 \\
&\leq (N+1)S^2 \left[L^2 \norm{\Delta x_N^k}^2 + \rho^2 \sum_{j = 1}^{N-1} \norm{A_N^\top A_j}^2 \left(\norm{\Delta x_j^k} + \norm{\Delta x_j^{k - 1}} \right)^2 + w_N^2\left(\norm{\Delta x_N^k} + \norm{\Delta x_N^{k-1}}\right)^2 \right] \\
&\leq (N+1)S^2 \left[\rho^2 \sum_{j = 1}^{N-1} \norm{A_N^\top A_j}^2 \left(\norm{\Delta x_j^k} + \norm{\Delta x_j^{k - 1}} \right)^2 + (L^2 + w_N^2)\left(\norm{\Delta x_N^k} + \norm{\Delta x_N^{k-1}}\right)^2 \right]
\end{align*}
where the first inequality was obtained by using $(a_1 + \ldots + a_N)^2 \leq N (a_1^2 + \ldots a_N^2)$ and the second inequality was obtained by using $L^2 \norm{\Delta x_N^k}^2 \leq L^2 \left(\norm{\Delta x_N^k} + \norm{\Delta x_N^{k-1}}\right)^2$.

\subsubsection{Boundedness}\label{sec:lyapunov_bounded}

\end{document}